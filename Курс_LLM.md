# *Курс Лекций: Большие языковые модели 2025. Современное состояния и технологии.*

# **Лекция_1: Введение в трансформеры и большие языковые модели (LLM)**  
---

## **1. Введение в курс**

### **Цели курса**
Этот курс призван:
- Объяснить **механизмы**, лежащие в основе современных больших языковых моделей.
- Показать, **как обучают LLM**, и где они применяются.
- Подготовить слушателей к работе в сфере NLP — как в исследованиях, так и в прикладных проектах.

Курс полезен:
- Будущим исследователям и ML-инженерам.
- Тем, кто хочет применять LLM в своих проектах.
- Специалистам из других областей, желающим понять, как работает генеративный ИИ.

### **Предпосылки (Prerequisites)**
- Базовые знания машинного обучения (нейросети, обучение моделей).
- Элементарная линейная алгебра (умножение матриц).

### **Формат и оценка**
- **2 единицы кредита**, можно сдавать на оценку или зачёт/незачёт.
- **Два экзамена**: промежуточный (5-я неделя) и финальный (вторая половина курса).
- Домашних заданий **нет** → 50% + 50% за экзамены.
- Записи лекций и слайды публикуются на сайте курса.
- Основной учебник: *"Super Study Guide — Transformers & LLMs"*.
- Дополнительно: [VIP Cheat Sheet](https://github.com/afshinea/stanford-cme295) (доступна на нескольких языках).

---

## **2. Краткий обзор NLP**

Обработка естественного языка (Natural Language Processing, NLP) — это область ИИ, занимающаяся анализом, пониманием и генерацией текста. Все задачи NLP можно разделить на три категории:

### **1. Классификация (один выход)**
- **Примеры**: анализ тональности («этот мишка милый» → *положительная*), определение языка, распознавание намерений.
- **Метрики**: точность (precision), полнота (recall), F1-мера (особенно важны при несбалансированных данных).

### **2. Мультиклассификация (много выходов)**
- **Пример**: распознавание именованных сущностей (NER): определение имён, дат, локаций в тексте.
- **Оценка**: на уровне токенов или типов сущностей.

### **3. Генерация (текст → текст)**
- **Примеры**: машинный перевод, ответы на вопросы (ChatGPT), суммаризация, генерация кода.
- **Метрики**:
  - **BLEU, ROUGE** — сравнение с эталонным переводом.
  - **Perplexity** — оценка «удивлённости» модели (чем ниже — тем лучше).
  - Современные LLM всё чаще используют **референс-свободные** подходы.

---

## **3. Представление текста: от токенизации до эмбеддингов**

### **Токенизация**
Модели обрабатывают не текст, а числа → нужна **дискретизация текста** на *токены*.

| Уровень         | Плюсы                              | Минусы                                  |
|------------------|-----------------------------------|----------------------------------------|
| **Слово**        | Простота                          | Большой словарь, OOV-проблема (слова вне словаря) |
| **Подслово**     | Уменьшает OOV, учитывает корни    | Увеличивает длину последовательности    |
| **Символ**       | Устойчив к опечаткам              | Очень длинные последовательности, слабая семантика |

Современные LLM (например, BPE — Byte Pair Encoding) используют **подсловную токенизацию** как оптимальный компромисс.

---

### **Эмбеддинги (векторные представления)**
Цель — преобразовать токены в **векторы**, где **похожие по смыслу слова близки** в пространстве.

- **One-Hot Encoding** — не подходит: все векторы ортогональны.
- **Word2Vec** (2013) — обучает эмбеддинги через **proxy-задачи**:
  - *Skip-Gram*: из слова предсказываем контекст.
  - *CBOW*: из контекста предсказываем слово.
- **Контекстуализированные эмбеддинги** (в отличие от Word2Vec) зависят от предложения → решает проблему омонимии (например, «банк» в «река» vs «финансы»).

---

## **4. От RNN к трансформерам**

### **Рекуррентные нейросети (RNN)**
- Обрабатывают текст **последовательно**, храня скрытое состояние (hidden state).
- **Проблемы**:
  - **Исчезающий градиент** → плохо работают с длинными текстами.
  - Медленное обучение (нельзя распараллелить).

### **LSTM**
- Улучшение RNN: добавляет «ячейку памяти» (cell state) для долгосрочного хранения информации.
- Но всё равно страдает от последовательной природы.

### **Механизм внимания (Attention)**
- Позволяет **прямо связывать** любые части входного и выходного текста.
- Ключевая идея: при переводе слова «мишка» модель может «взглянуть» на «teddy bear» в исходном предложении.

---

## **5. Архитектура трансформера (2017, «Attention is All You Need»)**

Трансформер состоит из:
- **Энкодера** (обрабатывает входной текст).
- **Декодера** (генерирует выход).

### **Ключевые компоненты**
#### **1. Self-Attention (самовнимание)**
- Каждый токен «внимает» всем остальным.
- Использует три проекции:
  - **Query (Q)** — что ищем?
  - **Key (K)** — с чем сравниваем?
  - **Value (V)** — что берём?
- Выход: взвешенная сумма значений, где веса = схожесть Q и K.

#### **2. Multi-Head Attention**
- Несколько «голов» внимания → модель учится разным аспектам семантики (аналогия: фильтры в свёрточных сетях).
- Результаты конкатенируются и проецируются обратно.

#### **3. Позиционные эмбеддинги**
- Трансформер не знает порядка слов → добавляются **позиционные кодировки** (в оригинале — синусоидальные функции).

#### **4. Feed-Forward Network (FFN)**
- Полносвязный слой после внимания → добавляет нелинейность.

### **Декодер: две стадии внимания**
1. **Masked Self-Attention** — смотрит **только на предыдущие** токены (чтобы не «заглядывать в будущее»).
2. **Cross-Attention** — сопоставляет текущий вывод с входным текстом (через энкодер).

---

## **6. Практические детали**

### **Токенизация на практике**
- Добавляются специальные токены: **<BOS>** (начало), **<EOS>** (конец).
- Пример:  
  `"a cute teddy bear is reading"` →  
  `[<BOS>, a, cute, teddy, bear, is, reading, <EOS>]`

### **Обучение**
- **Label Smoothing** — вместо жёсткой метки (1,0,0…) используют (0.9, 0.05, 0.05…) → модель не переобучается, лучше обобщает.
- Используется **softmax + кросс-энтропия**.

### **Остановка генерации**
- Модель генерирует до тех пор, пока не выдаст токен **<EOS>**.

---

## **7. Заключение**

- Трансформеры заменили RNN благодаря **параллелизации** и **глобальному контексту**.
- Современные LLM — это **масштабированные трансформеры**, обученные на огромных корпусах текста.
- Курс поможет вам не только понять, **как это работает**, но и **как это использовать** в реальных задачах — от медицины до генерации кода.

Конечно! Ниже — структурированная лекция на русском языке по содержанию второй лекции курса **CME 295: Transformers and Large Language Models** (Stanford University). Лекция ориентирована на магистрантов и сочетает теоретические основы с практическими инсайтами.

---

# **Лекция 2: Эволюция архитектуры трансформера и модели на её основе**  

---

## **1. Повторение: что такое трансформер?**

Трансформер — это архитектура, предложенная в 2017 году в статье **«Attention is All You Need»**, и лежащая в основе всех современных больших языковых моделей (LLM).

- **Основной механизм** — **самовнимание (self-attention)**: каждый токен взаимодействует со всеми остальными.
- **Ключевые компоненты**:
  - **Query (Q)** — что ищем?
  - **Key (K)** — с чем сравниваем?
  - **Value (V)** — что берём?
- **Формула внимания**:  
  \[
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  \]

- **Много-головое внимание (Multi-Head Attention)**: несколько параллельных механизмов внимания, позволяющих модели учить разные аспекты семантики.

- **Архитектура**:
  - **Encoder** — обрабатывает входной текст.
  - **Decoder** — генерирует выход (например, перевод).

---

## **2. Позиционные эмбеддинги: как модели «понимают» порядок слов**

В отличие от RNN, трансформер не обрабатывает текст последовательно — он теряет информацию о позиции токенов. Чтобы компенсировать это, используют **позиционные эмбеддинги**.

### **2.1. Обучаемые позиционные эмбеддинги**
- Для каждой позиции (1, 2, …, max_len) есть свой вектор.
- **Плюсы**: модель сама учит полезные представления.
- **Минусы**:
  - Не обобщается на длины, не встречавшиеся при обучении (например, если обучали на последовательностях ≤512, нельзя обрабатывать 1000 токенов).
  - Риск переобучения на структуру обучающего корпуса.

### **2.2. Синусоидальные (фиксированные) позиционные эмбеддинги** *(оригинальный трансформер)*
- Используются формулы:
  \[
  PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad
  PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
  \]
- **Преимущество**: работает с **любой длиной последовательности**.
- **Интуиция**: близкие позиции — близкие векторы (высокая косинусная близость), далёкие — менее похожи.

### **2.3. Современные подходы**

#### **T5: Relative Position Bias**
- В attention-механизм добавляется **обучаемый смещение**, зависящее от **относительного расстояния** между токенами.
- Позволяет гибко моделировать локальную структуру текста.

#### **ALiBi (Attention with Linear Bias)**
- Использует **детерминированное** смещение: чем дальше токены — тем сильнее штраф в attention.
- Не требует обучения, хорошо работает при увеличении длины контекста.

#### **RoPE (Rotary Position Embeddings)** — **самый популярный в 2025 г.**
- Вместо добавления позиционного вектора — **вращает** Q и K на угол, зависящий от позиции.
- **Преимущества**:
  - Относительное расстояние между токенами **естественно кодируется** в скалярном произведении.
  - Хорошо обобщается на длинные последовательности.
  - Совместим с эффективными оптимизациями (например, KV-кэшированием).

> **Итог**: RoPE стал де-факто стандартом в современных LLM (Llama, Mistral и др.).

---

## **3. Нормализация слоёв: от LayerNorm к RMSNorm**

В трансформере после каждого подслоя (attention, FFN) применяется нормализация.

### **Оригинальный подход (Post-LN)**
- Сначала вычисляется output подслоя → складывается с входом → нормализуется.
- **Проблема**: нестабильное обучение, особенно в глубоких моделях.

### **Современный подход (Pre-LN)**
- Сначала нормализуется вход → подаётся в подслой → результат складывается с исходным входом.
- **Плюсы**: более стабильное и быстрое обучение.

### **RMSNorm (Root Mean Square Normalization)**
- Упрощённая версия LayerNorm:
  \[
  \text{RMSNorm}(x) = \frac{x}{\sqrt{\text{mean}(x^2) + \varepsilon}} \cdot \gamma
  \]
- **Плюсы**:
  - Не вычисляет среднее (только RMS).
  - Меньше параметров → быстрее обучение и инференс.
- **Используется** в Llama, Mistral и других современных LLM.

> **Почему нормализация важна?**  
> Решает проблему **внутреннего сдвига ковариат (internal covariate shift)** — стабилизирует распределение активаций между слоями.

---

## **4. Внимание: от полного к разреженному**

Полное самовнимание имеет сложность **O(n²)**, что делает его непрактичным для длинных текстов.

### **4.1. Разреженное внимание (Sparse Attention)**
- **Идея**: токен взаимодействует только с **соседями** в окне (например, ±256 токенов).
- **Пример**: **Longformer**, **BigBird**.

### **4.2. Sliding Window Attention** *(Mistral)*
- Каждый слой применяет локальное внимание.
- **Эффект "рецептивного поля"**: через несколько слоёв токен «видит» весь контекст (аналогично свёрткам в CNN).
- **Плюсы**: O(n) сложность, поддержка контекста до 32K+ токенов.

---

## **5. Совместное использование проекций: MQA и GQA**

При генерации текста ключи (K) и значения (V) **кэшируются** (KV-cache), чтобы не пересчитывать их на каждом шаге.

### **Проблема**
- При стандартном MHA (Multi-Head Attention) кэш имеет размер **h × n × d**, где *h* — число голов.
- Это дорого по памяти и скорости.

### **Решения**

| Метод | Описание | Эффект |
|------|--------|--------|
| **MHA** | Каждая голова — свои Q, K, V | Базовый, но дорогой |
| **MQA (Multi-Query Attention)** | Все головы — **общие K и V**, разные Q | ×8 меньше памяти, но немного хуже качество |
| **GQA (Grouped-Query Attention)** | Группы голов делят K и V (например, 8 голов → 2 группы) | Баланс скорости и качества |

> **Современные LLM (Llama-2, Mistral)** используют **GQA** как оптимальный компромисс.

---

## **6. Типы архитектур трансформеров**

Существует **три основных класса** моделей на основе трансформера:

### **6.1. Encoder-Decoder (Seq2Seq)**
- **Примеры**: Original Transformer, T5.
- **Применение**: машинный перевод, суммаризация, span-corruption.
- **T5 особенности**:
  - Все задачи формулируются как **текст → текст**.
  - Используется **span corruption**: маскируются фрагменты текста, модель их восстанавливает.
  - **ByT5**: токенизация на уровне **байтов** (размер словаря = 256).

### **6.2. Encoder-only**
- **Примеры**: BERT, RoBERTa, DistilBERT.
- **Применение**: классификация, NER, вопросы-ответы.
- **Ключевая особенность**: **двунаправленное внимание** (каждый токен видит всё предложение).

#### **BERT: Bidirectional Encoder Representations from Transformers**
- **Pre-training задачи**:
  1. **MLM (Masked Language Modeling)**: предсказать замаскированные токены (80% — `[MASK]`, 10% — оригинал, 10% — случайное слово).
  2. **NSP (Next Sentence Prediction)**: предсказать, идут ли два предложения друг за другом.
- **Специальные токены**:
  - `[CLS]` — для классификации (агрегирует всё предложение).
  - `[SEP]` — разделяет два предложения.
  - **Segment embeddings** — указывают, к какому предложению относится токен (A или B).
- **Fine-tuning**: добавляется линейный слой поверх `[CLS]` или токенов.

#### **Развитие BERT**
- **DistilBERT**: в 2 раза меньше слоёв, почти без потерь в качестве → **дистилляция знаний** от учителя (BERT) к студенту.
- **RoBERTa**: убирает NSP, использует **динамическое маскирование** и **больше данных** → лучше качество.

### **6.3. Decoder-only** *(современные LLM)*
- **Примеры**: GPT, Llama, Mistral.
- **Применение**: генерация текста, чат-боты.
- **Особенности**:
  - Только **маскированное самовнимание** (causal attention).
  - Обучение на **next-token prediction**.
  - **Encoder не используется** — вычислительный бюджет сосредоточен на декодере.

> **Почему декодер-онли доминирует?**  
> Задача next-token prediction — **универсальна**, легко масштабируется и идеально подходит для генеративных приложений.

---

## **7. Заключение**

- **Трансформер** остаётся фундаментом LLM, но его компоненты эволюционировали:
  - **RoPE** вместо синусоидальных позиций.
  - **RMSNorm** вместо LayerNorm.
  - **GQA/MQA** для эффективной генерации.
  - **Sliding Window Attention** для длинных контекстов.
- **Encoder-only модели** (BERT) — стандарт для задач понимания.
- **Decoder-only модели** — стандарт для генерации.

> «Современные LLM — это не новые архитектуры, а **оптимизированные трансформеры**, адаптированные под масштаб, скорость и качество.»

---

Конечно! Ниже — структурированная и подробная **лекция на русском языке** по содержанию третьей лекции курса **CME 295: Transformers and Large Language Models** (Stanford University). Лекция ориентирована на магистрантов и сочетает концептуальные основы с современными практиками разработки и применения больших языковых моделей (LLM).

---

# **Лекция 3: Большие языковые модели (LLM) — архитектура, генерация и оптимизация**  

---

## **1. Что такое LLM?**

### **Определение**
**Большая языковая модель (Large Language Model, LLM)** — это:
- **Языковая модель**, присваивающая вероятности последовательностям токенов (обычно через *next-token prediction*).
- **Большая** в трёх смыслах:
  1. **Размер модели**: от **миллиардов** параметров (часто — сотни миллиардов или триллионы).
  2. **Объём данных**: обучение на **сотнях миллиардов — триллионах токенов**.
  3. **Вычислительные ресурсы**: требует **множества GPU** и сложной инфраструктуры.

> **Важно**: Современные LLM — это **decoder-only архитектуры** (например, GPT, Llama, Mistral).  
> Модели типа **BERT** (encoder-only) **не считаются LLM**, так как не генерируют текст.

---

## **2. Архитектурные инновации: Mixture of Experts (MoE)**

### **Проблема**
Полносвязные LLM активируют **все параметры** на каждом шаге → высокая стоимость инференса.

### **Решение: MoE**
Идея — **не использовать всех экспертов сразу**, а выбирать подмножество.

- **Эксперты (Experts)** — независимые подсети (обычно **FFN-блоки**).
- **Роутер / Гейт (Router/Gating network)** — решает, какие эксперты активировать.
- **Токен-уровневое решение**: каждый токен может идти к разным экспертам.

### **Типы MoE**
| Тип | Описание | Применение |
|-----|---------|------------|
| **Dense MoE** | Все эксперты участвуют, но с разными весами | Редко используется |
| **Sparse MoE** | Только **top-k** экспертов (обычно **k=1 или 2**) | Стандарт в современных LLM |

> **Почему именно FFN?**  
> Внимание (attention) — относительно «лёгкое»; FFN содержит **большинство параметров** (проекции в пространства 4×–10× больше, чем скрытое состояние).

### **Проблемы и решения**
- **Routing Collapse**: модель всегда выбирает одних и тех же экспертов.
  - **Решение**: добавление **auxiliary loss**, поощряющего равномерное использование экспертов.
- **Noisy Gating**: добавление шума к выходам роутера → больше разнообразия.

### **Преимущества MoE**
- **Масштабируемость**: можно иметь триллионы параметров (например, **Switch Transformer** — 1.6T), но использовать лишь подмножество при инференсе.
- **Эффективность обучения**: такие модели **быстрее сходятся** (выше sample efficiency).

---

## **3. Генерация текста: как выбирается следующий токен?**

LLM выдаёт **распределение вероятностей** над словарём. Как из него получить текст?

### **3.1. Жадная декодировка (Greedy Decoding)**
- Выбираем токен с **максимальной вероятностью**.
- **Плюсы**: быстро, детерминировано.
- **Минусы**: низкое разнообразие, **локальный оптимум** (не обязательно лучшая последовательность в целом).

### **3.2. Beam Search**
- Сохраняем **k наиболее вероятных последовательностей** на каждом шаге.
- В конце выбираем **наиболее вероятную** (часто с нормализацией по длине).
- **Используется** в машинном переводе, суммаризации — там, где важна точность.
- **Минус**: всё ещё **мало креативности**.

### **3.3. Сэмплирование (Sampling)**
- Токен **выбирается случайно** согласно распределению.
- **Разнообразие и креативность** → основной метод в чат-ботах.

#### **Ограничения сэмплирования**
- **Top-k Sampling**: сэмплируем только из **k наиболее вероятных** токенов.
- **Top-p (Nucleus) Sampling**: выбираем наименьшее множество токенов, чьи вероятности в сумме ≥ **p** (например, p=0.9).

---

## **4. Температура: контроль креативности**

Формула softmax с температурой **T**:
\[
P(x_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
\]

- **T → 0**: распределение становится **пикообразным** → жёсткая детерминированность.
- **T = 1**: стандартный softmax.
- **T → ∞**: распределение стремится к **равномерному** → максимальная креативность (и «хаос»).

> **Практика**:  
> - **T = 0.1–0.5** — для точных, фактических ответов.  
> - **T = 0.7–1.2** — для творческих задач (поэзия, сценарии).

> **Примечание**: Даже при T=0 инференс может быть **недетерминированным** из-за особенностей GPU-вычислений (порядок операций с плавающей точкой).

---

## **5. Стратегии подсказок (Prompting)**

### **5.1. In-Context Learning (ICL)**
- **Zero-shot**: просто задаём вопрос → модель отвечает.
- **Few-shot**: даём **примеры** вход/выход → повышает качество.

> **Современный тренд**: с ростом качества LLM, **хорошие инструкции** часто работают лучше, чем few-shot примеры.

### **5.2. Chain-of-Thought (CoT)**
- Просим модель **обосновать ответ** перед его выдачей:
  > *«Джону 10 лет. Через 5 лет ему будет 15. Ответ: 15.»*
- **Плюсы**:
  - Улучшает результаты в логических/математических задачах.
  - Даёт **интерпретируемость** → можно отлаживать рассуждения.

### **5.3. Self-Consistency**
- Генерируем **несколько CoT-ответов** → выбираем **наиболее частый финальный ответ**.
- Повышает **надёжность** без изменения самой модели.

---

## **6. Оптимизация инференса**

### **6.1. KV Cache**
- При генерации токен **t** должен обратиться ко всем предыдущим.
- Но **ключи (K) и значения (V)** предыдущих токенов **не меняются** → можно кэшировать.
- **Экономия**: не пересчитываем K/V при каждом шаге.

> **Примечание**: при обучении KV-cache **не используется** (все токены подаются сразу — *teacher forcing*).

### **6.2. Grouped-Query Attention (GQA)**
- Вместо отдельных K/V для каждой головы → **разделяем K/V между группами голов**.
- **Выгода**: уменьшает размер **KV-cache**, ускоряет генерацию.

### **6.3. PagedAttention (vLLM)**
- Проблема: при длинных запросах много **неиспользуемой памяти** (фрагментация).
- Решение: выделяем память **блоками фиксированного размера** (как виртуальная память ОС).
- **Результат**: до **2× эффективнее использования GPU-памяти**.

### **6.4. Multi-Latent Attention (DeepSeek)**
- **Сжимает K/V** через низкоранговую факторизацию.
- **Общая компрессия** для всех голов и для K/V → **кардинальное уменьшение кэша**.
- **Бонус**: иногда **улучшает качество** (регуляризация).

---

## **7. Ускорение генерации: спекулятивная декодировка**

### **Идея**
Использовать **маленькую «черновую» модель (draft)** для предсказания нескольких токенов → проверить их **большой «целевой» моделью**.

### **Процесс**
1. Draft-модель генерирует **k токенов** быстро.
2. Целевая модель оценивает их **одним проходом**.
3. Принимаем токен, если его вероятность в целевой модели ≥ вероятности в draft-модели.
4. Если токен отклонён — останавливаемся и генерируем заново.

> **Результат**: **2–5× ускорение** без потери качества (распределение итогового текста совпадает с чистым LLM).

### **Multi-Token Prediction**
- Draft-модель **встроена** в ту же архитектуру (дополнительные головы на выходе).
- Обучается на **предсказании нескольких токенов вперёд**.

---

## **8. Контекстная деградация (Context Rot)**

- При увеличении длины контекста (до 100K+ токенов) **качество извлечения информации снижается**.
- Особенно если **ключевой факт «закопан» среди шума** (*needle-in-a-haystack*).
- **Вывод**: длинный контекст ≠ лучше. Важно **точно подбирать релевантный контекст** (например, через RAG).

---

## **9. Заключение**

Современные LLM — это не просто «большие трансформеры», а **сложные системы**, сочетающие:
- **Архитектурные инновации** (MoE, RoPE, GQA),
- **Продуманные стратегии генерации** (CoT, sampling с температурой),
- **Инженерные оптимизации** (KV-cache, PagedAttention, speculative decoding).

> «Цель — не просто генерировать текст, а **делать это быстро, точно и контролируемо**».

---



---

# **Лекция 4: Обучение больших языковых моделей (LLM)**  

---


## **2. Почему обучение LLM — это не «обычное» обучение?**

Раньше для каждой задачи (спам-фильтрация, анализ тональности и т.д.) обучали отдельную модель **с нуля**.

Современный подход — **transfer learning**:
1. **Предобучение (pre-training)** на гигантских объёмах текста → модель учится «понимать» язык.
2. **Тонкая настройка (fine-tuning)** под конкретную задачу → используем уже обученные веса.

> LLM — это не модель для одной задачи, а **универсальный языкознатель**, которого затем «воспитывают» для конкретных ролей (ассистент, программист, редактор и т.д.).

---

## **3. Предобучение (Pre-training)**

### **Цель**
Модель учится предсказывать следующий токен:  
**Input**: "Мой плюшевый мишка…" → **Output**: "читает".

### **Данные**
- **Источники**: Common Crawl, Википедия, Reddit, GitHub, Stack Overflow.
- **Объём**:  
  - GPT-3: **300 млрд токенов**.  
  - Llama 3: **15 трлн токенов**.

> Это эквивалент многократного прочтения всей Википедии.

### **Архитектура**
- **Decoder-only трансформер** (90% современных LLM).
- **Next-token prediction** — основная задача.

### **Вычислительная сложность**
- Измеряется в **FLOPs** (floating-point operations).
- Обучение LLM: **~10²⁵ FLOPs**.
- FLOPS (с заглавной S) — **скорость вычислений** (FLOPs/сек), зависит от GPU/TPU.

---

## **4. Законы масштабирования и Chinchilla Law**

В 2020 году в работе *«Scaling Laws for Neural Language Models»* было показано:
- Больше данных + больше параметров → лучше качество.
- **Большие модели эффективнее**: за тот же объём данных дают лучшее качество.

### **Chinchilla Law (2022)**
Оптимальное соотношение между размером модели (**P**) и объёмом данных (**D**):  
\[
D \approx 20 \cdot P
\]

**Пример**:  
- GPT-3: 175B параметров, но обучена всего на 300B токенах → **недообучена**.  
- Llama 3: 405B параметров, обучена на 15T токенах → следует Chinchilla Law.

> Вывод: лучше обучать **меньшую модель дольше**, чем большую — коротко.

---

## **5. Практические вызовы предобучения**

1. **Стоимость**: от **миллионов до сотен миллионов долларов**.
2. **Экологический след**: огромное потребление энергии.
3. **Knowledge cutoff**: модель не знает ничего, что произошло **после даты обрезки данных**.  
   *(Например, GPT-5 знает события до 30 сентября 2025 г.)*
4. **Плагиат**: модель может дословно воспроизвести текст из обучающего набора.

---

## **6. Оптимизации обучения**

### **6.1. Параллелизация**

#### **Data Parallelism (DP)**
- Разбивает **батч** между GPU.
- Каждый GPU хранит **копию модели**.
- **Минус**: высокая стоимость связи (градиенты агрегируются между устройствами).

#### **ZeRO (Zero Redundancy Optimizer)**
- Устраняет дублирование данных между GPU:
  - **ZeRO-1**: шардинг состояний оптимизатора.
  - **ZeRO-2**: + шардинг градиентов.
  - **ZeRO-3**: + шардинг параметров модели.
- **Плюс**: позволяет обучать модели, не помещающиеся в память одного GPU.

#### **Model Parallelism**
- Разбивает **саму модель** между GPU:
  - **Tensor Parallelism**: разрезает матрицы умножений.
  - **Pipeline Parallelism**: каждый GPU обрабатывает свой блок слоёв.
  - **Expert Parallelism** (для MoE): каждый эксперт — на отдельном GPU.

---

### **6.2. FlashAttention**

**Проблема**:  
- Self-attention — O(n²) по памяти.  
- GPU имеет **быструю (SRAM)** и **медленную (HBM)** память.

**Решение**:  
- **Tiling**: разбиваем матрицы на блоки, умещающиеся в SRAM.
- Вычисляем **softmax частями** с корректирующим коэффициентом.
- **Результат**: до **10× меньше чтений/записей** в HBM → **ускорение + экономия памяти**.

**Дополнительный бонус**:  
- В backward pass **не храним активации**, а пересчитываем их → ещё больше экономии.

---

### **6.3. Квантование и смешанная точность**

#### **Квантование**
- Замена **FP32** (32 бита) → **FP16/BF16** (16 бит) → **INT8/INT4** (8/4 бита).
- **Плюсы**: меньше памяти, быстрее вычисления.
- **Минус**: потеря точности → риск деградации качества.

#### **Mixed Precision Training**
- **Веса хранятся в FP32** (точность важна для накопления ошибок).
- **Вычисления в FP16** (быстрее, меньше памяти).
- **Обновление весов в FP32** → сохраняем стабильность.

> Современные GPU (H100) дают до **4× прироста скорости** при переходе с FP32 на FP16.

---

## **7. Супервизированная тонкая настройка (Supervised Fine-Tuning, SFT)**

### **Зачем нужно?**
Предобученная модель — отличный «автор», но **плохой ассистент**:  
> Вопрос: *«Можно ли стирать плюшевого мишку в стиральной машине?»*  
> Ответ предобученной модели: *«Плюшевые мишки обычно сделаны из хлопка и полиэстера…»*  
> Ответ SFT-модели: *«Лучше стирать вручную, чтобы не повредить набивку.»*

### **Как работает**
- **Данные**: пары **(инструкция, идеальный ответ)**.
- **Формат**:  
  ```
  [INSTRUCTION] Как стирать мишку?  
  [RESPONSE] Рекомендуется ручная стирка в тёплой воде...
  ```
- **Loss**: только по токенам ответа (не по инструкции!).

### **Объём данных**
- GPT-3 SFT: **13 тыс. примеров**.  
- Llama 3 SFT: **10 млн примеров**.  
→ На **несколько порядков меньше**, чем при предобучении.

### **Состав данных**
- Творческие задачи (рассказы, стихи).
- Объяснения, списки, код.
- **Безопасность**: примеры отказов на вредоносные запросы.
- **Hedging**: смягчение утверждений (*«Возможно…», «Обычно…»*).

> Современные датасеты часто генерируются **другими LLM** + проверка людьми.

---

## **8. Оценка качества LLM**

### **8.1. Бенчмарки**
- **MMLU**: massive multitask → знания в 50+ областях.
- **GSM8K**: математические задачи.
- **HumanEval**: генерация кода.

**Проблема**:  
Модели могут **переобучаться на формат бенчмарков**, а не на суть задачи.

### **8.2. Chatbot Arena (Elo-рейтинг)**
- Пользователи сравнивают ответы двух моделей → выбирают лучший.
- **Минусы**:
  - Субъективность (эмодзи = хорошо или плохо?).
  - Смещение против «отказов» по соображениям безопасности.
  - Уязвимость к манипуляциям (можно определить модель по ответу на *«Кто ты?»*).

> **Вывод**: нет единой метрики. Качество = баланс между **фактичностью, полезностью, безопасностью**.

---

## **9. Эффективная тонкая настройка: LoRA и QLoRA**

### **9.1. LoRA (Low-Rank Adaptation)**
- **Идея**: не обновлять все веса, а обучать **низкоранговую добавку**:
  \[
  W_{\text{новый}} = W_0 + \Delta W = W_0 + B \cdot A
  \]
  где \(W_0\) — замороженные предобученные веса, \(A, B\) — маленькие матрицы (ранг **r = 4–64**).

- **Плюсы**:
  - На **99% меньше обучаемых параметров**.
  - Можно хранить **несколько адаптеров** для разных задач.
  - Лучше работает на **FFN-слоях**, а не на внимании.

- **Особенности**:
  - Learning rate в **10× выше**, чем при обычном обучении.
  - Маленькие батчи → лучше сходимость.

### **9.2. QLoRA**
- **Квантование + LoRA**:
  - \(W_0\) квантуется в **4-битный формат NF4** (оптимизированный под нормальное распределение весов).
  - \(A, B\) обучаются в **BF16**.
- **Результат**:  
  - **16× меньше VRAM**.  
  - Качество ≈ полному fine-tuning.

> QLoRA позволяет тонко настраивать 70B-модели даже на **одном потребительском GPU**.

---

## **10. Заключение**

Обучение LLM — это **многоэтапный процесс**:
1. **Предобучение**: «прочитать весь интернет» → понять структуру языка.
2. **SFT**: научить отвечать на вопросы, писать код, соблюдать этику.
3. **(Далее)**: Preference Tuning (RLHF/DPO) → выровнять под предпочтения человека.

**Ключевые тренды**:
- **Эффективность**: ZeRO, FlashAttention, LoRA.
- **Масштаб**: Chinchilla Law → больше данных, умеренный размер.
- **Безопасность**: вшитая в данные, а не через регулярки.

> «Мы больше не обучаем модели — мы **воспитываем** их.»

---

**Рекомендуемые материалы**:
- Оригинальные статьи:  
  - *Chinchilla: Training Compute-Optimal Large Language Models*  
  - *FlashAttention: Fast and Memory-Efficient Exact Attention*  
  - *LoRA: Low-Rank Adaptation of Large Language Models*  
  - *QLoRA: Efficient Finetuning of Quantized LLMs*


---

---

# **Лекция 5: Настройка LLM под предпочтения человека (Preference Tuning)**  
**Курс CME 295 — Stanford University**  
Подробная и структурированная **лекция на русском языке** по содержанию пятой лекции курса **CME 295: Transformers and Large Language Models** (Stanford University). Лекция посвящена **настройке больших языковых моделей под предпочтения человека** и охватывает ключевые методы: **RLHF**, **Best-of-N** и **DPO**.

---

## **1. Введение и повторение**

### **Что мы уже знаем?**
К настоящему моменту в курсе мы изучили **два ключевых этапа обучения LLM**:

1. **Предобучение (Pre-training)**  
   - Обучение на гигантских объёмах текста (сотни миллиардов — триллионы токенов).  
   - Задача: **next-token prediction**.  
   - Результат: модель «понимает» язык и код, но умеет только **автодополнять текст**.

2. **Супервизированная тонкая настройка (SFT, Supervised Fine-Tuning)**  
   - Обучение на качественных парах *инструкция → ответ*.  
   - Цель: превратить модель в **полезного ассистента**.  
   - Пример:  
     > *«Предложи активность для мишки» → «Почитай ему сказку!»*

> **Проблема**: даже после SFT модель может вести себя **некорректно, грубо или небезопасно**.

---

## **2. Зачем нужна настройка по предпочтениям?**

### **Пример неудачного ответа после SFT**
> **Вопрос**: *«Предложи новую активность для моего плюшевого мишки»*  
> **Ответ**: *«Я бы посоветовал тебе вообще не проводить время с мишкой»*

Этот ответ **грамматически корректен**, но **не соответствует ожиданиям пользователя**.

### **Почему SFT недостаточно?**
1. **Сложность сбора данных**: написать идеальный ответ для каждой ситуации — дорого и трудоёмко.
2. **Риск переобучения**: добавление отдельных примеров может сместить распределение ответов.
3. **Нет негативного сигнала**: SFT учит **только тому, что делать**, но не тому, **чего избегать**.

> **Решение**: вместо того чтобы говорить модели *«вот так отвечай»*, скажем:  
> *«Вот два ответа — первый лучше второго»*.

---

## **3. Сбор данных предпочтений**

### **Три подхода**
| Метод | Описание | Практическое применение |
|------|----------|------------------------|
| **Pointwise** | Оценка каждого ответа по шкале (0–1) | Сложно: человеку трудно ставить точные оценки |
| **Pairwise** | Сравнение двух ответов: *«какой лучше?»* | **Стандарт**: проще и надёжнее |
| **Listwise** | Ранжирование списка ответов | Редко используется из-за сложности |

### **Как генерируются пары?**
1. **Модель генерирует два разных ответа** на один запрос (с использованием температуры > 0).
2. **Эксперт (человек или другая LLM)** выбирает лучший.
3. Получаем **preference pair**:  
   - **Winner**: *«Мишки — замечательные компаньоны!»*  
   - **Loser**: *«Не трать время на мишку»*

> **Альтернатива**: переписать плохой ответ вручную.

---

## **4. RLHF: обучение с подкреплением на основе человеческой обратной связи**

### **Общая схема**
RLHF состоит из **двух этапов**:

#### **Этап 1: обучение модели вознаграждения (Reward Model, RM)**
- **Вход**: *запрос + ответ*  
- **Выход**: скалярный **score** (чем выше — тем лучше)
- **Формула (Bradley-Terry)**:
  \[
  P(y_w \succ y_l \mid x) = \sigma\big(r(x, y_w) - r(x, y_l)\big)
  \]
  где \(\sigma\) — сигмоида, \(r\) — функция вознаграждения.

- **Loss-функция** (negative log-likelihood):
  \[
  \mathcal{L} = -\mathbb{E}\left[\log \sigma\big(r(x, y_w) - r(x, y_l)\big)\right]
  \]

> **Важно**: RM — **точечная (pointwise)** модель, хотя обучается на парах.

#### **Этап 2: настройка политики через RL (PPO)**
- **Агент** = наша LLM  
- **Состояние** = текущий текст запроса  
- **Действие** = генерация следующего токена  
- **Вознаграждение** = выход RM на полный ответ

> **Цель**: максимизировать вознаграждение, **не отклоняясь сильно** от исходной (SFT) модели.

---

## **5. PPO: Proximal Policy Optimization**

### **Loss-функция PPO**
\[
\mathcal{L}^{\text{PPO}} = \mathbb{E}\left[ \min\left( r_t(\theta) \hat{A}_t,\ \text{clip}(r_t(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}_t \right) \right] - \beta \cdot \text{KL}(\pi_\theta \| \pi_{\text{ref}})
\]

где:
- \(r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\text{old}}(a_t \mid s_t)}\) — отношение политик
- \(\hat{A}_t\) — **advantage** (насколько действие лучше среднего)
- \(\text{KL}\) — дивергенция Ку́лбака — Ле́йблера от **базовой модели** (SFT)
- \(\beta, \varepsilon\) — гиперпараметры контроля стабильности

### **Зачем нужны ограничения?**
1. **Catastrophic forgetting**: не потерять знания, полученные при предобучении.
2. **Reward hacking**: модель не должна «обманывать» RM (например, вставлять ключевые слова).
3. **Стабильность обучения**: избежать резких обновлений весов.

> **Пример reward hacking**:  
> Цель — информативная лекция → метрика — громкость аплодисментов → модель рассказывает анекдоты.

---

## **6. Альтернативы: Best-of-N и DPO**

### **6.1. Best-of-N (BoN) — «лучший из N»**
- **Идея**: генерировать **N ответов**, выбрать **лучший по RM**.
- **Плюсы**:  
  - Не требует RL.  
  - Простая реализация.
- **Минусы**:  
  - **Дорого на инференсе**: нужно генерировать N раз.  
  - Если модель плоха — все ответы плохи.

> **Когда использовать?**  
> При низкой нагрузке и высоком качестве базовой модели.

---

### **6.2. DPO: Direct Preference Optimization**

#### **Проблемы RLHF**
- Сложность: нужно обучать **4 модели** (политика, RM, value function, базовая).
- Нестабильность: чувствительность к гиперпараметрам.
- Двухэтапность: если RM плох — всё обучение насмарку.

#### **Идея DPO**
- **Прямая оптимизация предпочтений** без RM и RL.
- **Loss-функция**:
  \[
  \mathcal{L}^{\text{DPO}} = -\mathbb{E}\left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)} \right) \right]
  \]

- **Инсайт статьи**: *«Ваша LLM — это скрытая модель вознаграждения»*  
  Вознаграждение можно **явно выразить через политику**, избавившись от RM.

#### **Преимущества DPO**
- **Проще**: только два набора весов — обучаемый и референсный.
- **Стабильнее**: супервизированное обучение, а не on-policy RL.
- **Дешевле**: нет необходимости в value function и сложных RL-трюках.

#### **Недостатки**
- **Ниже качество**, чем у PPO (особенно на сложных задачах).
- **Чувствителен к сдвигу распределения**: если preference-данные сильно отличаются от SFT — возможна деградация.

> **Практический выбор**:  
> - **DPO** — быстро, надёжно, «достаточно хорошо».  
> - **PPO** — максимум качества при наличии экспертов по RL.

---

## **7. Пример: от SFT к Preference Tuning**

> **Вопрос**: *«Можно ли стирать плюшевого мишку в стиральной машине?»*

- **После SFT**:  
  *«Нет, он может повредиться. Лучше стирать вручную»* → **фактически верно, но сухо**.

- **После Preference Tuning**:  
  *«Лучше не рисковать — мишке может быть больно! Нежная ручная стирка — самый заботливый выбор»* →  
  **те же факты, но с эмпатией и тёплым тоном**.

> **Ключевой вывод**: Preference Tuning **не добавляет знаний**, а **корректирует стиль и тон**.

---

## **8. Заключение**

- **Preference Tuning** — третий ключевой этап в создании современных LLM.
- **RLHF (PPO)** — мощный, но сложный метод, дающий наивысшее качество.
- **DPO** — элегантная альтернатива, упрощающая pipeline без катастрофической потери качества.
- **Best-of-N** — простой workaround для сценариев с низкой нагрузкой.

> «Мы уже научили модель **знать** и **говорить**. Теперь научим её **чувствовать** — то, как хочет слушать человек».

---

**Рекомендуемые материалы**:
- Оригинальные статьи:  
  - *Christiano et al., «Deep Reinforcement Learning from Human Preferences»* (RLHF)  
  - *Rafailov et al., «Direct Preference Optimization»* (DPO)
- Бенчмарк: [RewardBench](https://huggingface.co/spaces/allenai/reward-bench)
- Курс: [CME 295 — Transformers & LLMs (Stanford)](https://stanford-cme295.github.io)

---







---

# **Лекция 6: Рассуждающие большие языковые модели (Reasoning LLMs)**  
**Курс CME 295 — Stanford University**  
Ниже — подробная и структурированная **лекция на русском языке** по содержанию шестой лекции курса **CME 295: Transformers and Large Language Models** (Stanford University). Лекция посвящена **рассуждающим (reasoning) большим языковым моделям** — одному из самых актуальных направлений в ИИ на 2024–2025 гг.

---

## **1. Введение и контекст**

### **Что мы уже знаем?**
К настоящему моменту мы изучили:
1. **Предобучение (Pre-training)** — обучение на гигантских корпусах текстов с задачей *next-token prediction*. Модель учится «понимать» язык и код.
2. **Супервизированная тонкая настройка (SFT, Supervised Fine-Tuning)** — обучение модели быть полезным ассистентом с помощью пар *инструкция → идеальный ответ*.
3. **Настройка по предпочтениям (Preference Tuning)** — выравнивание поведения модели под человеческие оценки (например, через RLHF).

### **Проблема «обычных» LLM**
Несмотря на успехи, традиционные LLM имеют **четыре ключевых ограничения**:
1. **Ограниченные рассуждения**: плохо справляются с многошаговыми задачами (математика, сложный код).
2. **Статическое знание**: всё, что произошло **после даты cutoff**, модели неизвестно.
3. **Бездействие**: модель только отвечает, но не может выполнять действия (например, оформить заказ).
4. **Сложность оценки**: сложно измерить качество свободно генерируемого текста.

> **Фокус лекции**: как преодолеть первое ограничение — **улучшить способность модели к логическим рассуждениям**.

---

## **2. Что такое «рассуждение» и reasoning-модели?**

### **Определение**
**Рассуждение (reasoning)** — способность решать задачу через **многошаговый процесс**, разбивая её на подзадачи.

- **Non-reasoning вопрос**:  
  *«Какой курсовой код у Stanford Transformers and LLMs?»* → **CME 295** (фактический ответ).
- **Reasoning вопрос**:  
  *«Мишке из игрушки 5 лет в 2025 году. В каком году он родился?»* → требует вычислений и логики.

### **Ключевая идея: Chain-of-Thought (CoT)**
Вместо мгновенного ответа модель **сначала рассуждает**, потом даёт ответ:

> *«Сейчас 2025 год. Мишке 5 лет. Значит, он родился в 2025 – 5 = 2020 году. Ответ: 2020.»*

> **Важно**: reasoning-модель генерирует **не только ответ, но и цепочку рассуждений**.

---

## **3. История и практическое применение**

### **Хронология прорывов**
- **Сентябрь 2024**: OpenAI выпускает **o1-preview** — первую заметную reasoning-модель.
- **Декабрь 2024**: Google анонсирует **Gemini 2.0 Flash Thinking**.
- **Январь 2025**: DeepSeek публикует **R1**, показывая, как достичь сопоставимого качества **открытым способом**.
- **2025**: Модели от Anthropic, xAI, Mistral и др. внедряют reasoning-возможности.

### **Как это выглядит для пользователя?**
- В UI ChatGPT появляется надпись **«Thinking…»**.
- На самом деле модель генерирует **невидимую (или сокращённую) цепочку рассуждений**.
- **Вы платите не только за ответ, но и за «мышление»** — за все сгенерированные токены.

> **Зачем скрывать CoT?**  
> — Пользователь не хочет читать страницы рассуждений.  
> — Сырой CoT может быть нечитаемым или содержать ошибки.

---

## **4. Как оценивать reasoning-способности?**

### **Бенчмарки**
| Область | Бенчмарк | Описание |
|--------|----------|----------|
| **Математика** | **MATH**, **GSM8K**, **AIME** | Школьные и олимпиадные задачи |
| **Программирование** | **HumanEval**, **CodeForces**, **SWE-bench** | Генерация/исправление кода по условию |

### **Метрика: pass@k**
- Генерируем **k решений** одной задачи.
- **pass@k** = вероятность, что **хотя бы одно** решение верно.
- Пример: `pass@10` = из 10 попыток хотя бы одна прошла все тесты.

> **Почему не просто точность?**  
> В рассуждениях важна не только правильность, но и **возможность решить задачу при нескольких попытках** (особенно в коде!).

### **Роль температуры**
- **Низкая (T=0.1)**: решения похожи, но предсказуемы → `pass@k` почти не растёт с ростом k.
- **Средняя (T=0.6–0.8)**: баланс разнообразия и качества → максимум `pass@k`.
- **Высокая (T>1.0)**: хаотичные ответы → падение качества.

---

## **5. Как обучать reasoning-модели?**

### **Почему не SFT?**
- Писать качественные цепочки рассуждений **очень дорого**.
- Человеческое рассуждение ≠ оптимальное для модели.
- Есть **объективный сигнал вознаграждения** (правильный ответ → +1, неверный → 0).

> **Вывод**: используем **обучение с подкреплением (RL)**!

### **Архитектура обучения**
1. **База**: предобученная LLM (например, DeepSeek-V3).
2. **Reward-функция**:
   - **Формат**: есть ли токены `<think>...</think>` и `<answer>...</answer>`?
   - **Содержание**: верен ли окончательный ответ (проверка тестами или сравнением с эталоном)?
3. **RL-алгоритм**: **GRPO** (Group Relative Policy Optimization).

---

## **6. GRPO — ключевой алгоритм для рассуждений**

### **Проблема PPO**
Традиционный PPO (Proximal Policy Optimization) требует **обучения value-функции**, что:
- Добавляет сложность.
- Медленно сходится.

### **Идея GRPO (2024)**
- Для одного запроса генерируем **g решений** (например, g=8).
- Вычисляем **вознаграждение** для каждого.
- **Преимущество (advantage)** =  
  \[
  A_i = \frac{R_i - \mu_R}{\sigma_R}
  \]
  где \(R_i\) — вознаграждение i-го решения, \(\mu_R, \sigma_R\) — среднее и СКО по группе.

> **Суть**: модель учится не «абсолютно хорошо», а **лучше других своих попыток**.

### **Преимущества GRPO**
- Не нужна value-функция.
- Устойчив к сложности задачи (если все решения плохи, но одно чуть лучше — оно будет усилено).
- Лучше масштабируется.

---

## **7. Проблема «чрезмерного мышления»**

### **Что происходит при обучении?**
- С ростом RL-итераций **длина ответа растёт** (модель «перемудривает»).
- После определённого момента **качество стабилизируется**, но длина продолжает расти → **неэффективность**.

### **Причина в loss-функции GRPO**
В оригинальной формуле:
\[
\mathcal{L} \propto \frac{1}{|O_i|} \cdot A_i
\]
где \(|O_i|\) — длина i-го ответа.

- **Короткие плохие ответы** штрафуются **сильнее**, чем длинные плохие.
- Модель учится **удлинять ответы**, чтобы снизить штраф.

### **Решения**
1. **DAPO (2025)**: нормализация не по длине ответа, а глобально.
2. **Dr. GRPO**: полный отказ от деления на длину.
3. **Адаптивное мышление**: классификатор решает, нужна ли CoT для данного запроса.

---

## **8. Практический кейс: DeepSeek R1**

### **Этап 1: R1-Zero (доказательство концепции)**
- Берём **предобученную модель** (без SFT!).
- Применяем **GRPO с верифицируемым reward'ом**.
- **Результат**: резкий рост на бенчмарках (AIME, HumanEval), но CoT содержит ошибки (смешение языков, синтаксис).

### **Этап 2: полный пайплайн R1**
1. **Cold-start SFT**: люди исправляют плохие CoT из R1-Zero → качественный датасет.
2. **GRPO с улучшенным reward'ом**:
   - Форматирование.
   - Языковая согласованность (например, только английский).
   - Корректность ответа.
3. **Дополнительный SFT**: микс reasoning- и non-reasoning-данных (3:1).
4. **Финальный RL**: добавление компонентов **helpfulness** и **harmlessness**.

> **Итог**: R1 сопоставим с закрытыми моделями (o1, Claude), но с открытым методом.

---

## **9. Дистилляция reasoning-знаний**

### **Проблема**
- RL-обучение reasoning-моделей **дорого**.
- Как получить лёгкую reasoning-модель?

### **Решение: Distillation without SFT**
1. **Учитель** (R1) генерирует **пары**:  
   *запрос → <think>…</think><answer>…</answer>*.
2. **Ученик** (маленькая модель) обучается **воспроизводить всю последовательность** (а не вероятности токенов).

> **Результат**:  
> Модель **R1-Mini** (меньше 10B параметров) сопоставима с **o1-mini** по качеству.

---

## **10. Заключение**

- **Reasoning-модели** — следующий шаг в эволюции LLM.
- **Ключевые компоненты**:
  - Цепочка рассуждений (CoT).
  - Обучение с подкреплением (GRPO).
  - Верифицируемые reward'ы.
- **Тренды**:
  - **Эффективность**: борьба с «чрезмерным мышлением».
  - **Доступность**: дистилляция в лёгкие модели.
  - **Интеграция**: reasoning + RAG + агенты → реальные действия.

> «Модель больше не просто отвечает — она **думает вслух**, чтобы найти лучшее решение.»

---

**Рекомендуемые материалы**:
- DeepSeek R1: [arXiv:2501.xxxxx](https://arxiv.org/abs/2501.xxxxx)
- GRPO: *Group Relative Policy Optimization for Reasoning* (2024)
- DAPO, Dr. GRPO — улучшения GRPO (март 2025)
- HumanEval, GSM8K — бенчмарки для практики

---

---

# **Лекция 7: Интеграция LLM с внешним миром — RAG, вызов инструментов и агенты**  
**Курс CME 295 — Stanford University**  
Подробная и структурированная **лекция на русском языке** по содержанию седьмой лекции курса **CME 295: Transformers and Large Language Models** (Stanford University). Лекция посвящена **практическим методам интеграции больших языковых моделей (LLM) с внешними системами** — включая **RAG**, **вызов инструментов (tool calling)** и **агентские архитектуры**.

---

## **1. Введение и повторение**

### **Что мы уже знаем?**
К настоящему моменту в курсе мы:
1. Изучили архитектуру трансформера и её модификации.
2. Научились обучать LLM: предобучение → SFT → настройка по предпочтениям (RLHF/DPO).
3. Познакомились с **reasoning-моделями** — LLM, которые **рассуждают перед ответом** (цепочка рассуждений → ответ).

### **Проблемы «обычных» LLM**
Несмотря на успехи, у традиционных LLM остаются ключевые ограничения:
1. **Статическое знание**: всё, что произошло **после даты cutoff**, модели неизвестно.
2. **Пассивность**: модель только отвечает, но **не может выполнять действия** (отправить письмо, проверить погоду и т.д.).

> **Фокус лекции**: как преодолеть эти ограничения и превратить LLM в активного помощника, способного **взаимодействовать с внешним миром**.

---

## **2. RAG: Retrieval-Augmented Generation**

### **2.1. Зачем нужен RAG?**
- **Проблема**: LLM обучена на данных до определённой даты (например, GPT-5 — до 30 сентября 2024 г.).
- **Решение**: вместо дообучения — **динамически подгружать актуальную информацию** в контекст.

> **RAG** = **R**etrieval (поиск) + **A**ugmentation (дополнение контекста) + **G**eneration (генерация ответа).

---

### **2.2. Как работает RAG? Три этапа**

#### **Этап 1: Поиск релевантных документов (Retrieval)**
1. **База знаний**: коллекция документов (статьи, отчёты, инструкции).
2. **Чанкинг**: разбиение документов на фрагменты (обычно 300–500 токенов).
3. **Эмбеддинги**: каждый чанк преобразуется в вектор с помощью энкодера (например, **Sentence-BERT**).
4. **Поиск кандидатов**:
   - **Семантический поиск**: cosine similarity между эмбеддингами запроса и чанков.
   - **Ключевые слова**: **BM25** — эвристический метод, учитывающий частоту слов.
   - **Гибридный подход**: комбинация BM25 + эмбеддингов.

#### **Этап 2: Реранжирование (Re-ranking)**
- **Проблема**: топ-100 из первого этапа может содержать шум.
- **Решение**: использовать **кросс-энкодер** (cross-encoder):
  - Запрос и чанк подаются **вместе** в модель (BERT-like).
  - Модель выдаёт **релевантность** на основе совместного анализа.
- **Почему лучше?**: учитывает глубокие семантические связи, которые би-энкодер (bi-encoder) может упустить.

#### **Этап 3: Генерация ответа**
- Релевантные чанки добавляются в промпт:
  > *«Кто победил на выборах 5 ноября? Вот информация: [фрагмент 1], [фрагмент 2]…»*
- LLM генерирует ответ, **опираясь на подгруженные данные**.

---

### **2.3. Продвинутые техники RAG**

#### **Контекстуализация чанков**
- **Проблема**: чанк без контекста документа может быть непонятен.
- **Решение**: для каждого чанка генерируется **краткое резюме** всего документа с помощью LLM:
  > *«Этот фрагмент из отчёта о выборах 2024 года в Калифорнии…»*

#### **HyDE (Hypothetical Document Embeddings)**
- **Проблема**: запрос (вопрос) и документ (утверждение) имеют разную форму.
- **Решение**: сначала LLM генерирует **гипотетический документ-ответ**, затем ищет похожие реальные документы.

#### **Кэширование промптов (Prompt Caching)**
- При генерации резюме для тысяч чанков используется **один и тот же префикс**.
- Провайдеры (OpenAI, Anthropic) позволяют **кэшировать вычисления префикса** → экономия до **10×** на токены.

---

### **2.4. Оценка качества RAG**

| Метрика | Описание |
|--------|----------|
| **NDCG@k** | Учитывает **позицию** релевантных документов в топ-k (выше — лучше). |
| **MRR (Mean Reciprocal Rank)** | Обратный ранг **первого** релевантного документа. |
| **Precision@k** | Доля релевантных документов среди топ-k. |
| **Recall@k** | Какой процент **всех** релевантных документов попал в топ-k. |

> **Бенчмарк**: **MTEB** (Massive Text Embedding Benchmark) — стандарт для оценки ретриверов.

---

## **3. Вызов инструментов (Tool Calling)**

### **3.1. Зачем это нужно?**
- LLM не знает текущую погоду, курсы акций или расположение магазинов.
- **Идея**: предоставить модели **набор функций (инструментов)**, которые она может вызывать.

> **Пример**:  
> *«Найди плюшевого мишку рядом со мной»* → LLM вызывает `find_teddy_bear(location)`.

---

### **3.2. Как это работает? Три этапа**

#### **Этап 1: Определение функции**
- В промпт добавляется **спецификация API**:
  ```python
  def find_teddy_bear(lat: float, lon: float) -> List[TeddyBear]:
      """Ищет мишек в радиусе 1 км от координат."""
  ```

#### **Этап 2: Вызов функции**
- LLM генерирует **аргументы** для вызова:
  > *`find_teddy_bear(lat=37.42, lon=-122.17)`*
- Система выполняет вызов → получает структурированный ответ (JSON).

#### **Этап 3: Генерация финального ответа**
- Результат функции добавляется в контекст:
  > *«Вот мишки рядом: 1. "Мишка Тёпа" в магазине на Пало-Альто…»*
- LLM формирует **естественный ответ** для пользователя.

---

### **3.3. Обучение LLM вызывать инструменты**

#### **Способ 1: SFT (Supervised Fine-Tuning)**
- **Данные**: пары:
  - *Инпут*: «Найди мишку» + API-спецификация.
  - *Аутпут*: `find_teddy_bear(lat=..., lon=...)`.
- **Особенность**: нужны **две стадии обучения**:
  1. Генерация вызова функции.
  2. Преобразование JSON-ответа в текст.

#### **Способ 2: Промптинг + reasoning**
- **Идея**: не дообучать модель, а **объяснить в промпте**, как использовать инструменты.
- **Генерация объяснения**: используем другую LLM для написания инструкций на основе SFT-пар.

---

### **3.4. Проблема масштабируемости**

- **Проблема**: если у вас сотни инструментов, их описание не помещается в контекст.
- **Решение**: **Tool Selection** (выбор инструментов):
  1. LLM получает **список кратких описаний** инструментов.
  2. Выбирает **релевантные** (например, через few-shot промптинг).
  3. Только выбранные API добавляются в полный промпт.

> **Аналогия**: это **RAG для инструментов**.

---

### **3.5. Стандартизация: MCP (Model Context Protocol)**

- **Проблема**: каждый LLM (GPT, Claude, Llama) требует своего формата описания инструментов.
- **Решение**: **MCP** — открытый протокол от Anthropic:
  - **MCP Server**: предоставляет инструменты (например, база книг).
  - **MCP Client**: интегрируется в LLM (например, в Claude).

> **Пример**:  
> *«Порекомендуй стихотворение для мишки»* → LLM через MCP обращается к книжному API.

---

## **4. Агенты (Agents)**

### **4.1. Что такое агент?**
- **Определение**: система, которая **автономно преследует цель**, выполняя **многошаговые действия**.
- **Отличие от инструментов**: агенты **рассуждают итеративно**, выбирая действия на основе промежуточных результатов.

> **Пример**:  
> *«Мой мишка замёрз. Сделай что-нибудь»* → агент:
> 1. Узнаёт температуру в комнате (через инструмент `get_room_temp()`).
> 2. Видит, что 18°C — холодно.
> 3. Включает обогреватель (`set_heater(temp=22)`).
> 4. Подтверждает: *«Температура повышена до 22°C»*.

---

### **4.2. ReAct: Reason + Act**

Архитектура агента состоит из цикла:
1. **Observe**: интерпретировать текущее состояние мира.
2. **Plan**: наметить следующее действие.
3. **Act**: вызвать инструмент.

> **ReAct** — классический фреймворк для агентов (2022).

---

### **4.3. Мультиагентные системы**

- **Идея**: несколько агентов могут **взаимодействовать**:
  - Агент погоды → агент термостата → агент отчётов.
- **Стандартизация**: **Agent2Agent Protocol** (Google, 2024):
  - Определяет формат обмена: **навыки**, **статус**, **отмена**.

---

## **5. Безопасность**

### **5.1. Риски**
- **Утечка данных**: агент может отправить конфиденциальную информацию через публичный инструмент.
- **Вредоносные действия**: отправка спама, покупка товаров без подтверждения.

### **5.2. Меры защиты**
1. **На стадии обучения**:
   - Включение примеров **отказов от вредоносных запросов** в SFT и RLHF.
2. **На стадии инференса**:
   - **Safety Classifier**: блокирует опасные выходы.
   - **Ограничения на инструменты**: например, запрет на отправку email без подтверждения.

> **Реальный случай**: в 2024 г. хакеры использовали **агентские возможности Claude** для кибератаки. Anthropic опубликовала детальный отчёт.

---

## **6. Заключение и советы**

### **Ключевые выводы**
- **RAG** — решение для **актуальных знаний**.
- **Инструменты** — для **взаимодействия с API**.
- **Агенты** — для **сложных, многошаговых задач**.

### **Практические рекомендации**
1. **Начинайте с простого**: реализуйте один инструмент (например, поиск мишек).
2. **Используйте мощные модели**: сначала убедитесь, что задача вообще решаема.
3. **Отлаживайте через CoT**: анализируйте цепочку рассуждений агента.
4. **Не забывайте о безопасности**: даже простой агент может быть опасен.

> **Финальная мысль**:  
> *«Генерация кода стала дешёвой. Но умение **оценивать** код — остаётся ключевым навыком инженера».*

---

**Рекомендуемые материалы**:
- **RAG**: *«Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks»* (2020)
- **ReAct**: *«ReAct: Synergizing Reasoning and Acting in Language Models»* (2022)
- **MCP**: [Model Context Protocol (Anthropic)](https://github.com/anthropic/bedrock-mcp)
- **Безопасность**: *Agent Safety Bench*, отчёт Anthropic о кибератаке (ноябрь 2024)

---

